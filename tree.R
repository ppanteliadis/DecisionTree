###################################################################################################
# tree.R                                                                                          #
# @author:  Ioannis Pavlos Panteliadis <i.p.panteliadis@students.uu.nl>                           #                                           #
# @brief:   This file contains function declarations and implementations for the 1st assignment   #
#           in the Data Mining class of the Utrecht University.                                   #
###################################################################################################

# Using the data.tree package to represent our classification trees.
if (!require("data.tree")){
    install.packages("data.tree", dependencies = TRUE)
    library(data.tree)
}

# Generic function for the accuracy of a prediction
# algorithm.
#
# Inputs:
# =======
# t: A table generated by table()
accuracy <- function(t) {
    acc <- (sum(diag(t)) / sum(t)) * 100
    precision <- ((t[2,2]) / (t[2,2] + t[1,2])) * 100
    recall <-  ((t[2,2]) / (t[2,2] + t[2,1])) * 100
    
    print (t)
    print(paste("Accuracy: ", acc))
    print(paste("Precision: ", precision))
    print(paste("Recall: ", recall))
}

# Auxilary function to sample random columns from a matrix X
#
# Inputs:
# =======
# X:     A matrix to sample from
# n:     The number of columns to sample
#
# Return:
# =======
# Returns vector with indexes to the random columns
sample.random.columns <- function(X, n) {
    if (n == ncol(X)){
        return (sort(c(1:ncol(X)), decreasing = FALSE))
    }
    return (sort(c(sample(1:ncol(X), n, replace=F)), decreasing = FALSE))
}

# Computes and returns a vector of candidate splits for the selected 
# feature column and the class label.
# Follows the principal that candidate splits can only occur at the
# segments.
#
# Inputs:
# =======
# data: A dataframe to check for impurities
# class.label: The predictor class column in the data set
#
# Return:
# =======
# Returns a vector with the splits to consider on the given feature
candidate.splits <- function(data = c(), feature.label = 'V1', class.label = 'V2') {
    # sanity checks
    if (is.null(data)){
        stop("Feature table cannot be empty or null")
    }
    
    if (!(class.label %in% colnames(data))) {
        stop("Class label \'", class.label, "\' not in the dataset")
    }
    
    if (!(feature.label %in% colnames(data))) {
        stop("Feature label \'", feature.label, "\' not in the dataset")
    }
    
    if (c(feature.label) %in% c(class.label)) {
        stop("Feature column cannot be the same as the class label")
    }
    
    # First sort the data frame in the feature column
    data.sorted <- data[order(data[[feature.label]]), ]
    
    # Now find the segments
    # Secondly, find the unique values in the feature column
    unique.values <- unique(data.sorted[[feature.label]])
    
    # Now, compute the table of relative frequencies for the unique values
    # Will look like this:
    # x  value1    value2    value3   ...
    # P(1)   
    # P(0)
    relative.freq <- matrix(nrow = 2, ncol=length(unique.values))
    colnames(relative.freq) <- unique.values
    
    index <- 1
    for (val in unique.values) {
        P.1.given.val <- nrow(data[which(data[[class.label]] == 1 & data[[feature.label]] == val), ]) / nrow(data[data[[feature.label]] == val,])
        P.0.given.val <- 1 - P.1.given.val
        
        # Assign the values to the relative freq table
        relative.freq[1, index] <- P.1.given.val
        relative.freq[2, index] <- P.0.given.val
        
        index <- index + 1
    }
    
    # Now check for the segments in the relative freq table and register the splits to consider
    considered.splits <- c()
    
    
    for (index in 1:(length(unique.values) - 1)) {
        if (ncol(relative.freq) > 1){
            if (relative.freq[1, index] != relative.freq[1, index+1] & relative.freq[2, index] != relative.freq[2, index+1]) {
                # This is a segment
                num1 <- as.numeric(colnames(relative.freq)[index])
                num2 <- as.numeric(colnames(relative.freq)[index + 1])
                
                # Return the average of the 2 numbers
                considered.splits <- c(considered.splits, (num1 + num2)/2)
            }
        }
    }
    
    
    return (considered.splits)
}

# Calculates the impurity of a column using 
# the Gini-index.
# Inputs:
# =======
# data: A dataframe to check for impurities
# feature: The feature column name
# class.label: The predictor column name
# 
# Return:
# =======
# A vector with the first element being the best split for the feature 
# and the second element being the impurity it achieves (The smallest in the column)
impurity <- function(data = c(), feature = 'V1', class.label = 'V2', nmin = 2) {
    # sanity checks
    if (is.null(data)){
        stop("Feature table cannot be empty or null")
    }
    
    if (!(class.label %in% colnames(data))) {
        stop("Class label \'", class.label, "\' not in the dataset")
    }
    
    if (!(feature %in% colnames(data))) {
        stop("Feature label \'", feature, "\' not in the dataset")
    }
    
    if (c(feature) %in% c(class.label)) {
        stop("Feature column cannot be the same as the class label")
    }
    
    # Get the splits to consider
    considered.splits <- candidate.splits(data, feature, class.label)
    
    # calculate the Gini-index for this split
    impurities <- c()
    
    class <- data[, ncol(data)]

    for (split in considered.splits) {
        # First split the data set
        
        sub.data.1 <- which(data[[feature]] <= split)
        sub.data.2 <- which(data[[feature]] >  split)
        
        subset.x.1 <- data[sub.data.1, , drop = FALSE]
        subset.y.1 <- class[sub.data.1]
        
        subset.x.2 <- data[sub.data.2, , drop = FALSE]
        subset.y.2 <- class[sub.data.2]
        
        if (length(subset.y.1) >= nmin & length(subset.y.2) >= nmin){
            subset.1.class.1 <- which(subset.y.1 == 1)
            subset.1.class.0 <- which(subset.y.1 == 0)
            part1 <- (nrow(subset.x.1) / nrow(data)) * 
                (nrow(subset.x.1[subset.1.class.1,]) / nrow(subset.x.1)) * 
                (nrow(subset.x.1[subset.1.class.0,]) / nrow(subset.x.1))
            
            subset.2.class.1 <- which(subset.y.1 == 1)
            subset.2.class.0 <- which(subset.y.1 == 0)
            part2 <- (nrow(subset.x.2) / nrow(data)) * 
                (nrow(subset.x.2[subset.2.class.1,]) / nrow(subset.x.2)) * 
                (nrow(subset.x.2[subset.2.class.0,]) / nrow(subset.x.2))
            
            i <- part1 + part2
            
            impurities <- c(impurities, i)
        }
    }
    
    # Now select which impurity to return
    index.of.smallest.impurity <- which.min(impurities)
    return (c(considered.splits[index.of.smallest.impurity], impurities[index.of.smallest.impurity]))
    # which.min returns the index of the smallest value in the vector
}


# Creates a new node for the classification tree and returns it.
# 
# Inputs:
# =======
# df :       A dataframe to be held by the node in the classification tree.
# depth:     The depth of the created node. Set to the depth of the parent + 1.
# attribute: The attribute in check
# value:     The value in check
# child:     Binary. Either 'left' or 'right' must be specified.
# min.obs:   The minimum number of observations the node needs to have.
#
# Return:
# =======
# Returns a new node for the classification tree. Depending on whether this node is a 'left' or 'right' child,
# then the name of the node is assigned appropriately.
# ex. If 'attribute' is 'age', 'value' is '30' and 'child' is 'left', then the node will contain all the 
# cases age <= 30.
create.node <- function (node.label = "", node.type = "left", type = "binary", node.val = "", y = c()) {
    # Error checking
    if (type != "binary" && type != "numerical"){
        stop("Node can either be binary or numerical!")
    }
    
    if (node.type != "left" && node.type != "right" && node.type != "root") {
        stop("A node can either be left or right or root")
    }
    
    node <- Node$new()
    node$type <- node.type
    node$val <- node.val
    node$name <- node.label
    node$attr <- node.label
    node$y <- y
    
    if (node.type == "left"){
        node$name <- paste(node.label, "<=", node.val, sep = '')
    } else if (node.type == "right") {
        node$name <- paste(node.label, ">", node.val, sep = '')
    }
    
    # Increment the depth of the child = node of parent + 1
    # Maybe here perform some checks about the depth of the tree.
    node$isTerminal <- FALSE
    
    return (node)
}

# Auxiliary function to generate the Classification tree. We created this because the root
# of our Classification tree will contain the entire dataframe and it's label will be 'Classification Tree'.
# So, that first part had to be manually created.
# 
# Inputs:
# =======
# node:         Node to insert to
# X:            Dataframe for the tree
# Y:            A prediction class for X
# nmin:         Number of observations that a node must contain at least, for it to be allowed to be split
# minleaf:      Minimum number of observations required for a leaf node
#
# Return:
# =======
# Returns a classification tree.
tree.insert <- function(node = NULL, X = NULL, Y = NULL, nmin = 2, minleaf = 2) {
    # Sanity checks section
    if (is.null(node)) {
        stop("Cannot grow a tree from NULL parent")
    }
    
    if (is.null(X)) {
        stop("Feature vector can't be empty")
    }
    
    if (is.null(Y)) {
        stop("Class vector can't be empty")
    }
    
    # This node has the mininimum number of observations for it to be a leaf or it is a leaf already
    if (node$isTerminal || nrow(X) <= minleaf) {
        node$isTerminal <- TRUE
        pred.0 <- sum(Y==0)
        pred.1 <- sum(Y==1)
        
        if (pred.0 < pred.1) {
            node$pred <- 1
        } else {
            node$pred <- 0
        }
        return (node)
    }
    

    # Store the column names in a vector for later traversing
    nodelist <- colnames(X)
    # Merge the class column and feature matrix because that's how my alg (stupidly) works
    subsetted.data <- cbind(X, Y)

    # Initialize some variables
    max.i <- 0
    max.feat <- "V1"
    max.split <- 0
    # Find the best split in the subsetted data
    changed <- FALSE
    for (feat in nodelist){
        current.node <- feat
        nodelist <- nodelist[!nodelist %in% current.node]
        c.i <- impurity(data = as.data.frame(subsetted.data), 
                        feature = feat, 
                        class.label = colnames(subsetted.data)[ncol(subsetted.data)], 
                        nmin = nmin)
        split.val <- c.i[1]
        i <- c.i[2]
        
        # Find the node that achieves the greatest impurity reduction
        if (!is.null(i)){
            if (!is.na(i)){
                if (i > max.i) {
                    max.i <- i
                    max.feat <- current.node
                    max.split <- split.val
                    changed <- TRUE # Flag because sometimes the control never reached this assignment and it affected the split
                }
            }
        }
    }
    
    # Perform the split
    # Check if node contains enough observations to be allowed to be split..
    # And if we computed a candidate column split
    if (nrow(X) >= nmin & changed) {
        index.of.max.feature <- grep(paste("^", max.feat, "$", sep = ''), colnames(X))
        
        left.child.split  <- which(X[,index.of.max.feature] <= max.split)
        left.child.X <- X[left.child.split, ,drop = FALSE]
        left.child.Y <- Y[left.child.split]
        
        l.c <- create.node(node.label = max.feat, node.type = "left", node.val = max.split, y = left.child.Y)
        
        right.child.split <- which(X[, index.of.max.feature] > max.split)
        right.child.X <- X[right.child.split, ,drop = FALSE]
        right.child.Y <- Y[right.child.split]
        
        r.c <- create.node(node.label = max.feat, node.type = "right", node.val = max.split, y = right.child.Y)
       
        # A better split can't be computed. This check is done so the algorithm wont go in to stackoverflow due to enormous amount of recursions
        if (nrow(X) == nrow(left.child.X)){
            l.c$isTerminal <- TRUE
        }
        node$AddChildNode(
            tree.insert(node = l.c, X = left.child.X, Y = left.child.Y, nmin = nmin, minleaf = minleaf))
        
        if (nrow(X) == nrow(right.child.X)){
            r.c$isTerminal <- TRUE
        }
        node$AddChildNode(
            tree.insert(node = r.c, X = right.child.X, Y = right.child.Y, nmin = nmin, minleaf = minleaf))
        
    }
    
    return (node)
}


# Grows a classification tree based on a attribute matrix x, and a binary class labels y.
# 
# Input:
# ======
# x:        Data matrix contatining attribute values.
# y:        Vector of class labels (Binary)
# nmin:     Number of observations that a node must contain at least, for it to be allowed to be split
# minleaf:  Minimum number of observations required for a leaf node
# nfeat:    Number of features that should be considered for each split
# 
# Return:
# A classification tree object 
tree.grow <- function(x = c(), y = c(), nmin = 2, minleaf = 2, nfeat = ncol(x)) {
    # Sanity checks
    if (is.null(x)){
        stop("Feature table cannot be empty or null")
    }
    
    if (is.null(y)) {
        stop("Class label cannot be empty or null")
    }
    
    if (minleaf <= 1) {
        stop("Must have at least 2 observations on a leaf node")
    }
    
    if (nmin <= 0) {
        stop("Minimum number of observations for a node has to be positive")
    }
    
    if (nfeat > ncol(x)) {
        stop("Cannot take a sample larger than the population.")
    }
    
    # Get a vector of features to consider for a split
    features.to.consider <- sample.random.columns(x, nfeat)
    # Create a subset of the data
    subsetted.data.x <- x[, features.to.consider, drop = FALSE]
    
    
    # Create the tree's root node.
    root.node <- create.node(node.label = "Classification Tree", node.type = "root")
    root.node <- tree.insert(root.node, X = subsetted.data.x, Y = y, nmin = nmin, minleaf = minleaf)
    
    Do(root.node$leaves, function(node) node$isTerminal <- TRUE)
    
    return (root.node)
}

# Pre-order traversal of the tree in order to classify the attributes
#
# Inputs: 
# =======
# tr:   A classification tree created by tree.grow
# x:    A matrix to classify on the input tree
#
# Return:
# ========
# Returns a vector with the predicted values for x.
tree.classify <- function(tr = Node$new(), x = c()){
    if (is.null(tr)) {
        stop("Can't classify a null tree")
    }
    
    if (length(x) == 0) {
        stop("Can't classify an empty matrix")
    }
    
    # Initialize the return vector y
    y <- c()
    
    left.child <- tr$children[[1]]
    right.child <- tr$children[[2]]
    
    # Choose which path to take
    if (!is.null(left.child$attr)){
        for (i in 1:nrow(x)) {
            index.of.feature.column <- grep(paste("^", left.child$attr, "$", sep = ''), colnames(x))
            
            if (as.double(x[i, index.of.feature.column]) <= left.child$val) {
                y <- tree.traverse(node = left.child, row = x[i,], y = y, coln = colnames(x), rownum = i)
            } else {
                y <- tree.traverse(node = right.child, row = x[i,], y = y, coln = colnames(x), rownum = i)
            }
        }
    }
    return (y)
}

# Auxilary function to tree.classify to perform a pre-order traversal of classification tree
# created by tree.grow
#
# Inputs:
# =======
# node:     Node from the classification tree
# row:      Row of the dataframe in the node of the classification tree
# y:        If the node is a terminal node, then appends the prediction to y and returns it.
# coln:     The column names of the row
# rownum:   The number of the row from the dataframe. Used for error checking.
# 
# Return:
# =======
# A vector with the prediction values.
tree.traverse <- function(node = NULL, row = c(), y = c(), coln = c(), rownum = 1){
    if (node$isTerminal) {
        if (is.null(node$pred)) {
            pred.0 <- sum(node$y==0)
            pred.1 <- sum(node$y==1)
            
            if (pred.0 < pred.1) {
                node$pred <- 1
            } else {
                node$pred <- 0
            }
        }
        
        y <- c(y, node$pred)
        if (rownum != length(y)) {
            stop("Something's wrong. Row ", rownum, " y: ", length(y))
        }
        return (y)
    }
    
    left.child <- node$children[[1]]
    right.child <- node$children[[2]]
    
    index.of.feature.column <- grep(paste("^", left.child$attr, "$", sep = ''), coln)
    
    if (as.double(row[index.of.feature.column]) <= left.child$val) {
        # We move to the left child
        y <- tree.traverse(node = left.child, row = row, y = y, coln = coln, rownum = rownum)
    } else {
        # We move to the right child
        y <- tree.traverse(node = right.child, row = row, y = y, coln = coln, rownum = rownum)
    }
}




# Repeated application of tree.grow.
# 
# Inputs:
# ======
# x:        Data matrix containing the attribute values.
# y:        Vector of class labels
# nmin:     Number of observations that a node must contain at least
# minleaf:  Minimum number of observations required for a leaf node.
# nfeat:    Number of features that should be considered for each split
# m:        Number of trees to be returned
# 
# Return:
# ========
# Returns a list of Classification trees
tree.grow.bag <- function(x = c(), y = c(), nmin = 2, minleaf = 2, nfeat = ncol(x), m=100){
    # Some error checking
    if (minleaf == 1 || minleaf < 0) {
        stop("Must have at least 2 observations on a leaf node.")
    }
    
    if (nmin < 0) {
        stop("Observations have to be positive.")
    }
    
    new.tree.vec <- c()
    
    for (i in 1:m) {
        sampled.rows <- sample(nrow(x), nfeat)
        
        sampled.x <- x[sampled.rows, , drop = FALSE]
        sampled.y <- y[sampled.rows]
        
        new.tree.vec <- c(new.tree.vec,
                          tree.grow(x = sampled.x, y = sampled.y, nmin = nmin, minleaf = minleaf, nfeat = nfeat))
    }
    
    return (new.tree.vec)
}


# Repeated application of tree.grow.
# 
# Inputs:
# ======
# x:        Data matrix containing the attribute values.
# y:        Vector of class labels
# nmin:     Number of observations that a node must contain at least
# minleaf:  Minimum number of observations required for a leaf node.
# nfeat:    Number of features that should be considered for each split
# m:        Number of trees to be returned
# 
# Return:
# ========
# Returns a list of Classification trees
tree.grow.rf <- function(x = c(), y = c(), nmin = 2, minleaf = 2, nfeat = ncol(x), m=100){
    # Some error checking
    if (minleaf == 1 || minleaf < 0) {
        stop("Must have at least 2 observations on a leaf node.")
    }
    
    if (nmin < 0) {
        stop("Observations have to be positive.")
    }
    
    forest <- c()
    
    for (i in 1:m) {
        sampled.rows <- sample(nrow(x), nfeat, replace = TRUE)
        
        sampled.x <- x[sampled.rows, , drop = FALSE]
        sampled.y <- y[sampled.rows]
        
        tree <- tree.grow(x = sampled.x, y = sampled.y, nmin = nmin, minleaf = minleaf, nfeat = nfeat)
        forest <- c(forest, tree)
    }
    
    return (forest)
}


# Finally, the function tree.classify.bag takes as input a list of trees and a data matrix x 
# for which predictions are required. The function applies tree.classify 
# to x using each tree in the list in turn. For each row of x the final 
# prediction is obtained by taking the majority vote of the m classifications.
# 
# Inputs:
# =======
# list.of.trees:    A list of classification trees
# x:                A matrix to classify on the list of classification trees
# 
# Return:
# ========
# Returns a vector y, where y[i] contains the predicted class label for row i of x.
tree.classify.bag <- function(trees = c(), x=c()){
    if (length(trees) == 0) {
        stop("trees can't be empty")
    }
    
    if (length(x) == 0) {
        stop("x can't be an empty vector")
    }
    
    if (is.null(trees)) {
        stop("Can't classify a null tree")
    }
    
    if (length(x) == 0) {
        stop("Can't classify an empty matrix")
    }
    
    # z gets all values of the predictor from the random forest
    z <- c()
    
    # Initialize the return vector y
    y <- c()
    
    # Recursively call tree.classify for each tree in the list.
    for (tree in trees) {
        y_i <- tree.classify(tr = tree, x = x)
        y <- rbind(y, y_i)
    }
    
    # Calculate the majority vote for each column in the new dataframe
    for (col in 1:ncol(y)) {
        col.majority <- as.numeric(names(which.max(table(y[, col]))))
        z[col] <- col.majority
    }
    
    # Return the majority voted classifiers
    return (z)
}







######
# Pima data
######
data <- read.csv('pima.txt', header=FALSE)

index.of.class.column <- grep(paste("^", "V9", "$", sep = ''), colnames(data))
features <- as.matrix(data[, -index.of.class.column])
class <- data[,index.of.class.column]

tr.pima <- tree.grow(x = features, y = class)
pred.pima.ct <- tree.classify(tr = tr.pima, x = as.matrix(data[,1:8]))

accuracy(table(class, pred.pima.ct))
table(class, pred.pima.ct)

bag.pima <- tree.grow.bag(x = features, y = class, m=100)
pred.pima.bag <- tree.classify.bag(trees = bag.pima, x = as.matrix(data[,1:8]))



######
# Eclipse data
######
# Read the data in to dataframes
eclipse.train <- read.csv("eclipse-metrics-packages-2.0.csv", header = TRUE,  sep = ";")
eclipse.test <- read.csv("eclipse-metrics-packages-3.0.csv", header = TRUE, sep = ';')


# Change the values in the classifier column to binary. Namely, all values != 0 will be changed to 1.
eclipse.train$post[eclipse.train$post != 0] <- 1
eclipse.test$post[eclipse.test$post != 0]   <- 1

index.of.class.column.train <- grep(paste("^", "post", "$", sep = ''), colnames(eclipse.train))
index.of.class.column.test <- grep(paste("^", "post", "$", sep = ''), colnames(eclipse.test))


# Store the Classifier column in a variable
post.train <- as.matrix(eclipse.train[, index.of.class.column.train, drop = FALSE])
post.test  <- as.matrix(eclipse.test[, index.of.class.column.test, drop = FALSE])

eclipse.test$post <- NULL
eclipse.train$post <- NULL

# Remove the first 2 columns because they contain text.
# The drop argument is extremely important, as R will try to reduce the data types
# to the simplests data types possible
eclipse.train.features <- as.matrix(eclipse.train[,3:ncol(eclipse.train), drop = FALSE])
eclipse.test.features <- as.matrix(eclipse.test[,3:ncol(eclipse.test), drop = FALSE])

# Classification tree
tr.eclipse <- tree.grow(x = eclipse.train.features, y = post.train, nmin = 15, minleaf = 5, nfeat = 41)
plot(tr.eclipse)
pred.eclipse.ct <- tree.classify(tr = tr.eclipse, x = eclipse.test.features)
table(post.test, pred.eclipse.ct)
accuracy(table(post.test, pred.eclipse.ct))

# Bagging
bag.eclipse <- tree.grow.bag(x = eclipse.train.features, y = post.train, nmin = 15, minleaf = 5, nfeat = 41, m=100)
pred.eclipse.bag <- tree.classify.bag(trees = bag.eclipse, x = eclipse.test.features)
table(post.test, pred.eclipse.bag)
accuracy(table(post.test, pred.eclipse.bag))


# Random Forest
rf.eclipse <- tree.grow.rf(x = eclipse.train.features, y = post.train, nmin = 2, minleaf = 5, nfeat = 6, m=100)
pred.eclipse.rf <- tree.classify.bag(trees = rf.eclipse, x = eclipse.test.features)
table(post.test, pred.eclipse.rf)
accuracy(table(post.test, pred.eclipse.rf))


# Is the difference in accuracy significant?
# Classification Tree vs Bagging
ct.corr <- as.numeric(pred.eclipse.ct == post.test)
bag.corr <- as.numeric(pred.eclipse.bag == post.test)
ct.bag.comp <- table(ct.corr, bag.corr)
ct.bag.comp
pval(ct.bag.comp)

# Classification Tree vs Random Forest
ct.corr <- as.numeric(pred.eclipse.ct == post.test)
rf.corr <- as.numeric(pred.eclipse.rf == post.test)
ct.rf.comp <- table(ct.corr, rf.corr)
ct.rf.comp
pval(ct.rf.comp)

# Random Forest vs Bagging
rf.corr <- as.numeric(pred.eclipse.rf == post.test)
bag.corr <- as.numeric(pred.eclipse.bag == post.test)
rf.bag.comp <- table(rf.corr, bag.corr)
pval(rf.bag.comp)

pval <- function (t) {
    print (2*pbinom(t[1,1], sum(diag(t)), prob = 0.5))
}








